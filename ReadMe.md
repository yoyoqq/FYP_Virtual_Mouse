# 🎯 Virtual Mouse – Final Year Project  

A real-time **hand gesture recognition system** that replaces the physical mouse with computer vision and machine learning. The project enables smooth cursor movement, clicks, and control through hand gestures, aiming to improve accessibility and human–computer interaction (HCI).  

## 🚀 Features  
- 👋 **Hand Tracking** – Uses computer vision to detect and track hand landmarks in real time.  
- 🖱️ **Gesture Recognition** – Neural network–based classifier for mouse actions (click, scroll, drag).  
- ⚡ **Low Latency** – Optimized models reduce tracking latency to **< 5 ms**.  
- 🎨 **Lightweight UI** – Tkinter-based interface for demonstration and interaction.  
- 🏆 **Recognition** – Awarded *Best Innovation in Applied ML & HCI* at the **University of Greenwich Digital Shark Expo**.  

## 🛠️ Tech Stack  
- **Languages**: Python  
- **Libraries/Frameworks**: OpenCV, TensorFlow/Keras, MediaPipe, scikit-learn  
- **UI**: Tkinter  
- **Paradigms**: OOP design for modularity and extensibility  

## 📂 Project Structure  
```bash
FYP_Virtual_Mouse/
│── dataset/          # Collected hand gesture data  
│── models/           # Trained ML/DL models  
│── src/              # Main source code (gesture tracking, recognition, mouse control)  
│── ui/               # Tkinter-based demo UI  
│── utils/            # Helper functions  
│── requirements.txt  # Dependencies  
│── README.md         # Project documentation  


## ⚙️ Installation & Usage
git clone https://github.com/yoyoqq/FYP_Virtual_Mouse.git
cd FYP_Virtual_Mouse
pip install -r requirements.txt
python src/main.py



## 📊 Results
Gesture Recognition Accuracy: 97%
Latency: Reduced to under 5 ms
Expo: Showcased as a Top 5% Project in the University of Greenwich Expo

🔗 Links
Link: https://www.gre.ac.uk/digital-shark-expo/web-and-mobile-applications/yangan-yagol-xu-chen
